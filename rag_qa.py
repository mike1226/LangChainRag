# rag_qa.py

import config
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import GPT4All
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# 1. åŠ è½½åµŒå…¥æ¨¡å‹å’Œå‘é‡åº“
embedding_model = HuggingFaceEmbeddings(model_name=config.EMBEDDING_MODEL_PATH)
vectorstore = Chroma(persist_directory=config.VECTOR_DB_DIR, embedding_function=embedding_model)

# 2. åŠ è½½æœ¬åœ° LLM
llm = GPT4All(model=config.MODEL_PATH, verbose=False)

# 3. è‡ªå®šä¹‰ Promptï¼ˆå¯é€‰ï¼‰
prompt_template = PromptTemplate.from_template(
    "You are a helpful document assistant. Based on the following document excerpts, answer the user's question.\n\n"
    "Document content:\n{context}\n\n"
    "User's question: {question}\n"
    "Please answer concisely and accurately in English:"
)

# 4. æ„å»º RetrievalQA é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True  # ç”¨äºè°ƒè¯•æ£€ç´¢ç»“æœ
)

# 5. å¯åŠ¨é—®ç­” CLI
print("ğŸ¤– å¯åŠ¨å®Œæˆï¼Œè¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ exit é€€å‡ºã€‚")
while True:
    query = input("ğŸ§  è¯·è¾“å…¥é—®é¢˜ï¼š")
    if query.lower() in ("exit", "quit"):
        break
    result = qa_chain({"query": query})
    print(f"\nğŸ’¡ å›ç­”ï¼š{result['result']}\n")

    # Debugï¼ˆå¯é€‰ï¼šæ˜¾ç¤ºè¢«åŒ¹é…çš„æ–‡æ¡£ç‰‡æ®µï¼‰
    print("ğŸ” åŒ¹é…çš„æ–‡æ¡£ç‰‡æ®µï¼š")
    for doc in result["source_documents"]:
        print(f"---\n{doc.page_content[:500]}\n...")
